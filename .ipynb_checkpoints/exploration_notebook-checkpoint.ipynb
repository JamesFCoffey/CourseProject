{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python implementation of a causal topic modeling paper.\n",
    "\n",
    "This program implements the following paper:\n",
    "<blockquote>\n",
    "    <p>Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. 2013. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. DOI=10.1145/2505515.2505612</p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.api import VAR\n",
    "import shutil\n",
    "import tarfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from mp_functions import token_pipeline\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "pres_market = pd.read_csv(\"./data/PRES00_WTA.csv\")\n",
    "AAMRQ = pd.read_csv(\"./data/AAMRQ.csv\")\n",
    "AAPL = pd.read_csv(\"./data/AAPL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Contract</th>\n",
       "      <th>Units</th>\n",
       "      <th>$Volume</th>\n",
       "      <th>LowPrice</th>\n",
       "      <th>HighPrice</th>\n",
       "      <th>AvgPrice</th>\n",
       "      <th>LastPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/1/2000</td>\n",
       "      <td>Dem</td>\n",
       "      <td>224</td>\n",
       "      <td>112.043</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/1/2000</td>\n",
       "      <td>Reform</td>\n",
       "      <td>2</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5/1/2000</td>\n",
       "      <td>Rep</td>\n",
       "      <td>116</td>\n",
       "      <td>57.95</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/2/2000</td>\n",
       "      <td>Dem</td>\n",
       "      <td>87</td>\n",
       "      <td>44.369</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/2/2000</td>\n",
       "      <td>Reform</td>\n",
       "      <td>50</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>11/9/2000</td>\n",
       "      <td>Reform</td>\n",
       "      <td>2,065</td>\n",
       "      <td>2.062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>11/9/2000</td>\n",
       "      <td>Rep</td>\n",
       "      <td>10,055</td>\n",
       "      <td>542.973</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>11/10/2000</td>\n",
       "      <td>Dem</td>\n",
       "      <td>3,454</td>\n",
       "      <td>3,328.02</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>11/10/2000</td>\n",
       "      <td>Reform</td>\n",
       "      <td>23</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>11/10/2000</td>\n",
       "      <td>Rep</td>\n",
       "      <td>3,519</td>\n",
       "      <td>148.171</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date     Contract     Units     $Volume      LowPrice  \\\n",
       "0      5/1/2000          Dem       224     112.043         0.490   \n",
       "1      5/1/2000       Reform         2       0.067         0.019   \n",
       "2      5/1/2000          Rep       116       57.95         0.488   \n",
       "3      5/2/2000          Dem        87      44.369         0.501   \n",
       "4      5/2/2000       Reform        50       0.196         0.003   \n",
       "..          ...          ...       ...         ...           ...   \n",
       "571   11/9/2000       Reform     2,065       2.062         0.000   \n",
       "572   11/9/2000          Rep    10,055     542.973         0.025   \n",
       "573  11/10/2000          Dem     3,454    3,328.02         0.950   \n",
       "574  11/10/2000       Reform        23        0.02         0.000   \n",
       "575  11/10/2000          Rep     3,519     148.171         0.020   \n",
       "\n",
       "         HighPrice      AvgPrice      LastPrice  \n",
       "0            0.550         0.500          0.550  \n",
       "1            0.048         0.034          0.019  \n",
       "2            0.501         0.500          0.500  \n",
       "3            0.522         0.510          0.508  \n",
       "4            0.005         0.004          0.003  \n",
       "..             ...           ...            ...  \n",
       "571          0.001         0.001          0.000  \n",
       "572          0.109         0.054          0.050  \n",
       "573          0.980         0.964          0.969  \n",
       "574          0.001         0.001          0.000  \n",
       "575          0.071         0.042          0.031  \n",
       "\n",
       "[576 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pres_market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7/3/2000</td>\n",
       "      <td>26.63</td>\n",
       "      <td>26.63</td>\n",
       "      <td>26.00</td>\n",
       "      <td>26.13</td>\n",
       "      <td>26.13</td>\n",
       "      <td>483100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7/5/2000</td>\n",
       "      <td>27.25</td>\n",
       "      <td>28.88</td>\n",
       "      <td>27.06</td>\n",
       "      <td>28.38</td>\n",
       "      <td>28.38</td>\n",
       "      <td>1840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/6/2000</td>\n",
       "      <td>28.44</td>\n",
       "      <td>29.56</td>\n",
       "      <td>27.81</td>\n",
       "      <td>29.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7/7/2000</td>\n",
       "      <td>29.81</td>\n",
       "      <td>29.94</td>\n",
       "      <td>29.13</td>\n",
       "      <td>29.13</td>\n",
       "      <td>29.13</td>\n",
       "      <td>1150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7/10/2000</td>\n",
       "      <td>29.75</td>\n",
       "      <td>30.13</td>\n",
       "      <td>29.19</td>\n",
       "      <td>30.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>711800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>12/24/2001</td>\n",
       "      <td>21.72</td>\n",
       "      <td>21.73</td>\n",
       "      <td>20.77</td>\n",
       "      <td>21.19</td>\n",
       "      <td>21.19</td>\n",
       "      <td>1350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>12/26/2001</td>\n",
       "      <td>21.37</td>\n",
       "      <td>21.74</td>\n",
       "      <td>21.18</td>\n",
       "      <td>21.57</td>\n",
       "      <td>21.57</td>\n",
       "      <td>938900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>12/27/2001</td>\n",
       "      <td>21.35</td>\n",
       "      <td>21.79</td>\n",
       "      <td>21.20</td>\n",
       "      <td>21.50</td>\n",
       "      <td>21.50</td>\n",
       "      <td>1190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>12/28/2001</td>\n",
       "      <td>21.60</td>\n",
       "      <td>22.19</td>\n",
       "      <td>21.55</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>853000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>12/31/2001</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.64</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.30</td>\n",
       "      <td>22.30</td>\n",
       "      <td>1210000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>373 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date   Open   High    Low  Close  Adj Close   Volume\n",
       "0      7/3/2000  26.63  26.63  26.00  26.13      26.13   483100\n",
       "1      7/5/2000  27.25  28.88  27.06  28.38      28.38  1840000\n",
       "2      7/6/2000  28.44  29.56  27.81  29.00      29.00  1820000\n",
       "3      7/7/2000  29.81  29.94  29.13  29.13      29.13  1150000\n",
       "4     7/10/2000  29.75  30.13  29.19  30.00      30.00   711800\n",
       "..          ...    ...    ...    ...    ...        ...      ...\n",
       "368  12/24/2001  21.72  21.73  20.77  21.19      21.19  1350000\n",
       "369  12/26/2001  21.37  21.74  21.18  21.57      21.57   938900\n",
       "370  12/27/2001  21.35  21.79  21.20  21.50      21.50  1190000\n",
       "371  12/28/2001  21.60  22.19  21.55  22.00      22.00   853000\n",
       "372  12/31/2001  22.00  22.64  22.00  22.30      22.30  1210000\n",
       "\n",
       "[373 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AAMRQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7/3/2000</td>\n",
       "      <td>0.930804</td>\n",
       "      <td>0.969866</td>\n",
       "      <td>0.930804</td>\n",
       "      <td>0.952009</td>\n",
       "      <td>0.821251</td>\n",
       "      <td>70828800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7/5/2000</td>\n",
       "      <td>0.950893</td>\n",
       "      <td>0.985491</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.795256</td>\n",
       "      <td>265216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/6/2000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.945313</td>\n",
       "      <td>0.886161</td>\n",
       "      <td>0.925223</td>\n",
       "      <td>0.798145</td>\n",
       "      <td>309545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7/7/2000</td>\n",
       "      <td>0.939174</td>\n",
       "      <td>0.978795</td>\n",
       "      <td>0.930804</td>\n",
       "      <td>0.972098</td>\n",
       "      <td>0.838581</td>\n",
       "      <td>263603200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7/10/2000</td>\n",
       "      <td>0.965960</td>\n",
       "      <td>1.040179</td>\n",
       "      <td>0.959821</td>\n",
       "      <td>1.020089</td>\n",
       "      <td>0.879981</td>\n",
       "      <td>397796000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>12/21/2001</td>\n",
       "      <td>0.375179</td>\n",
       "      <td>0.384643</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.323494</td>\n",
       "      <td>256334400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>12/24/2001</td>\n",
       "      <td>0.373214</td>\n",
       "      <td>0.383036</td>\n",
       "      <td>0.373214</td>\n",
       "      <td>0.381429</td>\n",
       "      <td>0.329040</td>\n",
       "      <td>50629600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>12/26/2001</td>\n",
       "      <td>0.381250</td>\n",
       "      <td>0.398214</td>\n",
       "      <td>0.377500</td>\n",
       "      <td>0.383750</td>\n",
       "      <td>0.331042</td>\n",
       "      <td>146400800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>12/27/2001</td>\n",
       "      <td>0.385357</td>\n",
       "      <td>0.397321</td>\n",
       "      <td>0.385357</td>\n",
       "      <td>0.394107</td>\n",
       "      <td>0.339977</td>\n",
       "      <td>191508800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>12/28/2001</td>\n",
       "      <td>0.392321</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.392143</td>\n",
       "      <td>0.400536</td>\n",
       "      <td>0.345522</td>\n",
       "      <td>299124000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>373 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date      Open      High       Low     Close  Adj Close     Volume\n",
       "0      7/3/2000  0.930804  0.969866  0.930804  0.952009   0.821251   70828800\n",
       "1      7/5/2000  0.950893  0.985491  0.906250  0.921875   0.795256  265216000\n",
       "2      7/6/2000  0.937500  0.945313  0.886161  0.925223   0.798145  309545600\n",
       "3      7/7/2000  0.939174  0.978795  0.930804  0.972098   0.838581  263603200\n",
       "4     7/10/2000  0.965960  1.040179  0.959821  1.020089   0.879981  397796000\n",
       "..          ...       ...       ...       ...       ...        ...        ...\n",
       "368  12/21/2001  0.375179  0.384643  0.371429  0.375000   0.323494  256334400\n",
       "369  12/24/2001  0.373214  0.383036  0.373214  0.381429   0.329040   50629600\n",
       "370  12/26/2001  0.381250  0.398214  0.377500  0.383750   0.331042  146400800\n",
       "371  12/27/2001  0.385357  0.397321  0.385357  0.394107   0.339977  191508800\n",
       "372  12/28/2001  0.392321  0.410714  0.392143  0.400536   0.345522  299124000\n",
       "\n",
       "[373 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AAPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granger Test\n",
    "\n",
    "\"Granger tests...measur[e] statistical significance at different time lags using auto regression to identify causal relationships. Let $y_{t}$ and $x_{t}$ be two time series. To see if $x_{t}$ 'Granger causes' $y_{t}$ with maximum $p$ time lag, run the following regression:\n",
    "\n",
    "$$\n",
    "y_{t} = a_{0} + a_{1}y_{tâˆ’1} + ... + a_{p}y_{tâˆ’p} + b_{1}x_{tâˆ’1} + ... + b_{p}x_{tâˆ’p}\n",
    "$$\n",
    "\n",
    "Then, use F-tests to evaluate the significance of the lagged $x$ terms. The coefficients of lagged $x$ terms estimate the impact of $x$ on $y$. We average the $x$ term coefficients, $\\frac{\\sum_{i=1}^{p}b_{i}}{|b|}$, as an impact value.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAMRQ</th>\n",
       "      <th>AAPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.581667</td>\n",
       "      <td>-0.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.032738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.030506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.026414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>-0.026667</td>\n",
       "      <td>-0.001547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>-0.130000</td>\n",
       "      <td>0.004881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.006369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.243333</td>\n",
       "      <td>0.006369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.004524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>372 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AAMRQ      AAPL\n",
       "1    0.581667 -0.003906\n",
       "2    1.000000  0.006696\n",
       "3    0.540000  0.032738\n",
       "4    0.146667  0.030506\n",
       "5    0.500000  0.026414\n",
       "..        ...       ...\n",
       "368 -0.026667 -0.001547\n",
       "369 -0.130000  0.004881\n",
       "370  0.270000  0.006369\n",
       "371  0.243333  0.006369\n",
       "372  0.216667  0.004524\n",
       "\n",
       "[372 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close = pd.concat([AAMRQ[\"Close\"], AAPL[\"Close\"]], axis=1, keys=[\"AAMRQ\", \"AAPL\"])\n",
    "close = close.rolling(3, center=True, min_periods=2).mean()\n",
    "close = close.diff()[1:]\n",
    "close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=0.4416  , p=0.5068  , df_denom=368, df_num=1\n",
      "ssr based chi2 test:   chi2=0.4452  , p=0.5046  , df=1\n",
      "likelihood ratio test: chi2=0.4449  , p=0.5048  , df=1\n",
      "parameter F test:         F=0.4416  , p=0.5068  , df_denom=368, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 2\n",
      "ssr based F test:         F=0.5103  , p=0.6008  , df_denom=365, df_num=2\n",
      "ssr based chi2 test:   chi2=1.0345  , p=0.5961  , df=2\n",
      "likelihood ratio test: chi2=1.0331  , p=0.5966  , df=2\n",
      "parameter F test:         F=0.5103  , p=0.6008  , df_denom=365, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 3\n",
      "ssr based F test:         F=0.3828  , p=0.7655  , df_denom=362, df_num=3\n",
      "ssr based chi2 test:   chi2=1.1706  , p=0.7601  , df=3\n",
      "likelihood ratio test: chi2=1.1687  , p=0.7605  , df=3\n",
      "parameter F test:         F=0.3828  , p=0.7655  , df_denom=362, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 4\n",
      "ssr based F test:         F=1.0379  , p=0.3875  , df_denom=359, df_num=4\n",
      "ssr based chi2 test:   chi2=4.2557  , p=0.3725  , df=4\n",
      "likelihood ratio test: chi2=4.2313  , p=0.3756  , df=4\n",
      "parameter F test:         F=1.0379  , p=0.3875  , df_denom=359, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 5\n",
      "ssr based F test:         F=0.9640  , p=0.4398  , df_denom=356, df_num=5\n",
      "ssr based chi2 test:   chi2=4.9688  , p=0.4197  , df=5\n",
      "likelihood ratio test: chi2=4.9355  , p=0.4238  , df=5\n",
      "parameter F test:         F=0.9640  , p=0.4398  , df_denom=356, df_num=5\n"
     ]
    }
   ],
   "source": [
    "# Is first column \"caused by\" second column up to a given lag?\n",
    "gc_res = grangercausalitytests(close, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_vals = []\n",
    "for i in range(1, len(gc_res) + 1):\n",
    "    p_vals.append(gc_res[i][0]['params_ftest'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5067873616230052,\n",
       " 0.6007552098554052,\n",
       " 0.7654686970619549,\n",
       " 0.3874795496740359,\n",
       " 0.4398020489287572]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(p_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.subtract(1, p_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = np.subtract(1, p_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49321264, 0.39924479, 0.2345313 , 0.61252045, 0.56019795])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   260.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 23 Nov 2020</td> <th>  Prob (F-statistic):</th> <td>9.31e-45</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:37:28</td>     <th>  Log-Likelihood:    </th> <td> -233.09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   371</td>      <th>  AIC:               </th> <td>   470.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   369</td>      <th>  BIC:               </th> <td>   478.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.6428</td> <td>    0.040</td> <td>   16.150</td> <td> 0.000</td> <td>    0.565</td> <td>    0.721</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   -0.0061</td> <td>    0.024</td> <td>   -0.259</td> <td> 0.796</td> <td>   -0.053</td> <td>    0.040</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>161.879</td> <th>  Durbin-Watson:     </th> <td>   1.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3606.831</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.284</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>18.058</td>  <th>  Cond. No.          </th> <td>    1.69</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.414\n",
       "Model:                            OLS   Adj. R-squared:                  0.413\n",
       "Method:                 Least Squares   F-statistic:                     260.8\n",
       "Date:                Mon, 23 Nov 2020   Prob (F-statistic):           9.31e-45\n",
       "Time:                        20:37:28   Log-Likelihood:                -233.09\n",
       "No. Observations:                 371   AIC:                             470.2\n",
       "Df Residuals:                     369   BIC:                             478.0\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.6428      0.040     16.150      0.000       0.565       0.721\n",
       "const         -0.0061      0.024     -0.259      0.796      -0.053       0.040\n",
       "==============================================================================\n",
       "Omnibus:                      161.879   Durbin-Watson:                   1.785\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3606.831\n",
       "Skew:                          -1.284   Prob(JB):                         0.00\n",
       "Kurtosis:                      18.058   Cond. No.                         1.69\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc_res[1][1][0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Summary of Regression Results   \n",
       "==================================\n",
       "Model:                         VAR\n",
       "Method:                        OLS\n",
       "Date:           Mon, 23, Nov, 2020\n",
       "Time:                     20:37:28\n",
       "--------------------------------------------------------------------\n",
       "No. of Equations:         2.00000    BIC:                   -10.2797\n",
       "Nobs:                     367.000    HQIC:                  -10.4208\n",
       "Log likelihood:           909.784    FPE:                2.71597e-05\n",
       "AIC:                     -10.5138    Det(Omega_mle):     2.56020e-05\n",
       "--------------------------------------------------------------------\n",
       "Results for equation AAMRQ\n",
       "===========================================================================\n",
       "              coefficient       std. error           t-stat            prob\n",
       "---------------------------------------------------------------------------\n",
       "const           -0.009421         0.021853           -0.431           0.666\n",
       "L1.AAMRQ         0.800274         0.052984           15.104           0.000\n",
       "L1.AAPL          1.023943         1.797251            0.570           0.569\n",
       "L2.AAMRQ        -0.009810         0.065306           -0.150           0.881\n",
       "L2.AAPL         -2.897335         2.132583           -1.359           0.174\n",
       "L3.AAMRQ        -0.465091         0.060873           -7.640           0.000\n",
       "L3.AAPL         -0.247153         1.915933           -0.129           0.897\n",
       "L4.AAMRQ         0.367037         0.065179            5.631           0.000\n",
       "L4.AAPL          1.238642         2.121852            0.584           0.559\n",
       "L5.AAMRQ        -0.082454         0.052829           -1.561           0.119\n",
       "L5.AAPL          0.978018         1.783545            0.548           0.583\n",
       "===========================================================================\n",
       "\n",
       "Results for equation AAPL\n",
       "===========================================================================\n",
       "              coefficient       std. error           t-stat            prob\n",
       "---------------------------------------------------------------------------\n",
       "const           -0.000509         0.000646           -0.788           0.431\n",
       "L1.AAMRQ        -0.001446         0.001566           -0.923           0.356\n",
       "L1.AAPL          0.727527         0.053115           13.697           0.000\n",
       "L2.AAMRQ         0.002245         0.001930            1.163           0.245\n",
       "L2.AAPL          0.124881         0.063025            1.981           0.048\n",
       "L3.AAMRQ         0.000034         0.001799            0.019           0.985\n",
       "L3.AAPL         -0.529698         0.056622           -9.355           0.000\n",
       "L4.AAMRQ        -0.000027         0.001926           -0.014           0.989\n",
       "L4.AAPL          0.347019         0.062708            5.534           0.000\n",
       "L5.AAMRQ        -0.002460         0.001561           -1.576           0.115\n",
       "L5.AAPL          0.069139         0.052710            1.312           0.190\n",
       "===========================================================================\n",
       "\n",
       "Correlation matrix of residuals\n",
       "            AAMRQ      AAPL\n",
       "AAMRQ    1.000000  0.114764\n",
       "AAPL     0.114764  1.000000\n",
       "\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VAR(close)\n",
    "results = model.fit(5)\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impact_value(data, lag):\n",
    "    model = VAR(data)\n",
    "    results = model.fit(lag)\n",
    "    numerator = 0\n",
    "    for i in range(1, lag + 1):\n",
    "        numerator += results.params[results.params.columns.values[0]]['L' + str(i) + '.' + results.params.columns.values[1]]\n",
    "    return numerator / np.abs(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8733952397899155"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impact_value(close, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_purity(num_pos_impact, num_neg_impact, num_significant):\n",
    "    p_prob = num_pos_impact / num_significant\n",
    "    n_prob = num_neg_impact / num_significa\n",
    "    entropy = p_prob * np.log(p_prob) + n_prob * np.log(n_prob)\n",
    "    return 100 + 100 * entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import NYTAC\n",
    "\n",
    "Do not run this code below. It was to run the original data cleaning steps. Running again will delete the NYTAC in storage. It has been commented out for safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tars = []\n",
    "# for root, dirs, files in os.walk(\"./data/nyt_corpus/data\"):\n",
    "#     if dirs:\n",
    "#         delete = dirs.copy()\n",
    "#         delete[:] = [x for x in dirs if x not in ['2000', '2001', '2002', '2003']]\n",
    "#         dirs[:] = [x for x in dirs if x in ['2000', '2001', '2002', '2003']]\n",
    "#         for name in delete:\n",
    "#             subdir = os.path.join(root, name)\n",
    "#             with os.scandir(subdir) as it:\n",
    "#                 for entry in it:\n",
    "#                     os.remove(entry)\n",
    "#             os.rmdir(subdir)\n",
    "#     if files:\n",
    "#         if os.path.basename(root) == '2003':\n",
    "#             delete = files.copy()\n",
    "#             delete = [x for x in files if x not in ['01.tgz', '02.tgz', '03.tgz']]\n",
    "#             files[:] = [x for x in files if x in ['01.tgz', '02.tgz', '03.tgz']]\n",
    "#             for name in delete:\n",
    "#                 os.remove(os.path.join(root, name))\n",
    "#         for file in files:\n",
    "#             tars.append(os.path.join(root, file))\n",
    "\n",
    "# for file_path in tars:\n",
    "#     tar = tarfile.open(file_path)\n",
    "#     tar.extractall(path=os.path.dirname(file_path))\n",
    "#     tar.close()\n",
    "#     os.remove(file_path)\n",
    "\n",
    "# # collect articles for 2000 Presidential Election\n",
    "# with os.scandir(\"./data/nyt_corpus/data/2000\") as it:\n",
    "#     for entry in it:\n",
    "#         if os.path.basename(entry) in ['05', '06', '07', '08', '09', '10']:\n",
    "#             shutil.copytree(entry, os.path.join(\"./data/nyt_corpus/data/election/2000\", os.path.basename(entry)))\n",
    "\n",
    "# # collect articles for Stock Time Series, AAMRQ vs. AAPL\n",
    "# with os.scandir(\"./data/nyt_corpus/data/2000\") as it:\n",
    "#     for entry in it:\n",
    "#         if os.path.basename(entry) in ['07', '08', '09', '10', '11', '12']:\n",
    "#             shutil.copytree(entry, os.path.join(\"./data/nyt_corpus/data/stock/2000\", os.path.basename(entry)))\n",
    "# with os.scandir(\"./data/nyt_corpus/data/2001\") as it:\n",
    "#     for entry in it:\n",
    "#         shutil.copytree(entry, os.path.join(\"./data/nyt_corpus/data/stock/2001\", os.path.basename(entry)))\n",
    "\n",
    "# # collect articles for Iraq War\n",
    "# with os.scandir(\"./data/nyt_corpus/data/2002\") as it:\n",
    "#     for entry in it:\n",
    "#         shutil.copytree(entry, os.path.join(\"./data/nyt_corpus/data/war/2002\", os.path.basename(entry)))\n",
    "# with os.scandir(\"./data/nyt_corpus/data/2003\") as it:\n",
    "#     for entry in it:\n",
    "#         if os.path.basename(entry) in ['01', '02', '03']:\n",
    "#             shutil.copytree(entry, os.path.join(\"./data/nyt_corpus/data/war/2003\", os.path.basename(entry)))\n",
    "\n",
    "# # remove unused directories\n",
    "# for year in ['2000', '2001', '2002', '2003']:\n",
    "#     shutil.rmtree(os.path.join(\"./data/nyt_corpus/data\", os.path.basename(year)))\n",
    "\n",
    "# # initialize list of documents to delete\n",
    "# delete = []\n",
    "\n",
    "# # delete documents that do not contain \"Bush\" and \"Gore\" or do not contain document bodies\n",
    "# for root, dirs, files in os.walk(\"./data/nyt_corpus/data/election\"):\n",
    "#     if files:\n",
    "#         for name in files:\n",
    "#             tree = ET.parse(os.path.join(root, name))\n",
    "#             tree_root = tree.getroot()\n",
    "#             element = tree_root.find('./body/body.content/block[@class=\"full_text\"]')\n",
    "#             if element:\n",
    "#                 keep = 0\n",
    "#                 Bush = 0\n",
    "#                 Gore = 0\n",
    "#                 for para in element.findall('p'):\n",
    "#                     para_list = nltk.word_tokenize(para.text)\n",
    "#                     if 'Bush' in para_list:\n",
    "#                         Bush = 1\n",
    "#                     if 'Gore' in para_list:\n",
    "#                         Gore = 1\n",
    "#                     keep = Bush * Gore\n",
    "#                 if not keep:\n",
    "#                     delete.append(os.path.join(root, name))\n",
    "#             else:\n",
    "#                 delete.append(os.path.join(root, name))\n",
    "\n",
    "# # delete documents that do not contain document bodies\n",
    "# for root, dirs, files in os.walk(\"./data/nyt_corpus/data/stock\"):\n",
    "#     if files:\n",
    "#         for name in files:\n",
    "#             tree = ET.parse(os.path.join(root, name))\n",
    "#             tree_root = tree.getroot()\n",
    "#             element = tree_root.find('./body/body.content/block[@class=\"full_text\"]')\n",
    "#             if not element:\n",
    "#                 delete.append(os.path.join(root, name))\n",
    "\n",
    "# # delete documents that do not contain \"Iraq\" or do not contain document bodies\n",
    "# for root, dirs, files in os.walk(\"./data/nyt_corpus/data/war\"):\n",
    "#     if files:\n",
    "#         for name in files:\n",
    "#             tree = ET.parse(os.path.join(root, name))\n",
    "#             tree_root = tree.getroot()\n",
    "#             element = tree_root.find('./body/body.content/block[@class=\"full_text\"]')\n",
    "#             if element:\n",
    "#                 keep = 0\n",
    "#                 for para in element.findall('p'):\n",
    "#                     para_list = nltk.word_tokenize(para.text)\n",
    "#                     if 'Iraq' in para_list:\n",
    "#                         keep = 1\n",
    "#                 if not keep:\n",
    "#                     delete.append(os.path.join(root, name))\n",
    "#             else:\n",
    "#                 delete.append(os.path.join(root, name))\n",
    "\n",
    "# # delete the unneeded documents\n",
    "# for name in delete:\n",
    "#     os.remove(name)\n",
    "\n",
    "# # delete empty directories\n",
    "# for root, dirs, files in os.walk(\"./data/nyt_corpus/data\"):\n",
    "#     if not dirs and not files:\n",
    "#         os.rmdir(root)\n",
    "\n",
    "# # consolidate xml files into text files\n",
    "# # one text file contains the documents from the date in the file name\n",
    "# # documents are stored one document per line\n",
    "# for root, dirs, files in os.walk(\"./data/nyt_corpus/data\"):\n",
    "#     if files:\n",
    "#         yyyy = os.path.basename(os.path.dirname(os.path.dirname(root)))\n",
    "#         mm = os.path.basename(os.path.dirname(root))\n",
    "#         dd = os.path.basename(root)\n",
    "#         file_name = yyyy + \"-\" + mm + \"-\" + dd + \".txt\"\n",
    "#         base_folder = os.path.basename(os.path.dirname(os.path.dirname(os.path.dirname(root))))\n",
    "#         directory = os.path.join(\"./data/nyt_corpus/data\", base_folder)\n",
    "#         f = open(os.path.join(directory, file_name), \"w\")\n",
    "#         for name in files:\n",
    "#             tree = ET.parse(os.path.join(root, name))\n",
    "#             tree_root = tree.getroot()\n",
    "#             element = tree_root.find('./body/body.content/block[@class=\"full_text\"]')\n",
    "#             paragraphs = []\n",
    "#             for para in element.findall('p'):\n",
    "#                 paragraphs.append(para.text)\n",
    "#             f.write(\" \".join(paragraphs) + \"\\n\")\n",
    "#         f.close()\n",
    "\n",
    "# # remove unused directories\n",
    "# for subdir in [\"election/2000\", \"stock/2000\", \"stock/2001\", \"war/2002\", \"war/2003\"]:\n",
    "#     shutil.rmtree(os.path.join(\"./data/nyt_corpus/data\", subdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Set Up Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.datetime64('2018-02-01') - np.datetime64('2018-01-01')).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_corpus(self):\n",
    "#     \"\"\"\n",
    "#     Read document, fill in self.documents, a list of list of word\n",
    "#     self.documents = [[\"the\", \"day\", \"is\", \"nice\", \"the\", ...], [], []...]\n",
    "\n",
    "#     Update self.number_of_documents\n",
    "#     \"\"\"\n",
    "#     with os.scandir(self.documents_path) as it:\n",
    "#         s0 = time.time()\n",
    "#         for entry in it:\n",
    "#             e0 = time.time()\n",
    "#             print(\"time_0: {}\".format(e0 - s0))\n",
    "#             s0 = time.time()\n",
    "#             with open(entry, 'r') as file:\n",
    "#                 s1 = time.time()\n",
    "#                 while True:\n",
    "#                     e1 = time.time()\n",
    "#                     print(\"time_1: {}\".format(e1 - s1))\n",
    "#                     s1 = time.time()\n",
    "#                     line = file.readline()\n",
    "#                     if not line:\n",
    "#                         break\n",
    "#                     tokens = word_tokenize(line.lower())\n",
    "#                     tokens= [x for x in tokens if x.isalnum()]\n",
    "#                     nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "#                     tokens = [x for x in tokens if x not in nltk_stop_words]\n",
    "#                     tokens = normalise(tokens, variety=\"AmE\", verbose = False)\n",
    "#                     tokens = [WordNetLemmatizer().lemmatize(word, pos='v') for word in tokens]\n",
    "#                     tokens = [WordNetLemmatizer().lemmatize(word, pos='n') for word in tokens]\n",
    "#                     self.documents.append(tokens)\n",
    "#                     self.doc_timestamps.append(os.path.basename(entry).split(\".\")[0])\n",
    "#                     self.number_of_documents += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_matrix):\n",
    "    \"\"\"\n",
    "    Normalizes the rows of a 2d input_matrix so they sum to 1\n",
    "    \"\"\"\n",
    "\n",
    "    row_sums = input_matrix.sum(axis=1)\n",
    "    try:\n",
    "        assert (np.count_nonzero(row_sums)==np.shape(row_sums)[0]) # no row should sum to zero\n",
    "    except Exception:\n",
    "        raise Exception(\"Error while normalizing. Row(s) sum to zero\")\n",
    "    new_matrix = input_matrix / row_sums[:, np.newaxis]\n",
    "    return new_matrix\n",
    "\n",
    "class Corpus(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A collection of documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, documents_path):\n",
    "        \"\"\"\n",
    "        Initialize empty document list.\n",
    "        \"\"\"\n",
    "        self.documents = []\n",
    "        self.doc_timestamps = []\n",
    "        self.vocabulary = []\n",
    "        self.likelihoods = []\n",
    "        self.documents_path = documents_path\n",
    "        self.term_doc_matrix = None\n",
    "        self.document_topic_prob = None  # P(z | d)\n",
    "        self.topic_word_prob = None  # P(w | z)\n",
    "        self.topic_prob = None  # P(z | d, w)\n",
    "\n",
    "        self.number_of_documents = 0\n",
    "        self.vocabulary_size = 0\n",
    "\n",
    "    def build_corpus(self):\n",
    "        \"\"\"\n",
    "        Read document, fill in self.documents, a list of list of word\n",
    "        self.documents = [[\"the\", \"day\", \"is\", \"nice\", \"the\", ...], [], []...]\n",
    "\n",
    "        Update self.number_of_documents\n",
    "        \"\"\"\n",
    "        lines = []\n",
    "        with os.scandir(self.documents_path) as it:\n",
    "            for entry in it:\n",
    "                with open(entry, 'r') as file:\n",
    "                    while True:\n",
    "                        line = file.readline()\n",
    "                        if not line:\n",
    "                            break\n",
    "                        lines.append(line)\n",
    "                        self.doc_timestamps.append(os.path.basename(entry).split(\".\")[0])\n",
    "                        self.number_of_documents += 1\n",
    "        with mp.Pool() as pool:\n",
    "            self.documents = pool.map(token_pipeline, lines)\n",
    "\n",
    "    def build_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Construct a list of unique words in the whole corpus. Put it in self.vocabulary\n",
    "        for example: [\"rain\", \"the\", ...]\n",
    "\n",
    "        Update self.vocabulary_size\n",
    "        \"\"\"\n",
    "        word_set = set()\n",
    "        for document in self.documents:\n",
    "            for word in document:\n",
    "                word_set.add(word)\n",
    "        self.vocabulary = list(word_set)\n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "\n",
    "    def build_term_doc_matrix(self):\n",
    "        \"\"\"\n",
    "        Construct the term-document matrix where each row represents a document,\n",
    "        and each column represents a vocabulary term.\n",
    "\n",
    "        self.term_doc_matrix[i][j] is the count of term j in document i\n",
    "        \"\"\"\n",
    "        self.term_doc_matrix = np.zeros((self.number_of_documents, self.vocabulary_size))\n",
    "        for i, document in enumerate(self.documents):\n",
    "            for j, word in enumerate(self.vocabulary):\n",
    "                self.term_doc_matrix[i][j] = document.count(word)\n",
    "\n",
    "    def initialize_randomly(self, number_of_topics):\n",
    "        \"\"\"\n",
    "        Randomly initialize the matrices: document_topic_prob and topic_word_prob\n",
    "        which hold the probability distributions for P(z | d) and P(w | z): self.document_topic_prob, and self.topic_word_prob\n",
    "\n",
    "        Don't forget to normalize!\n",
    "        HINT: you will find numpy's random matrix useful [https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.random.html]\n",
    "        \"\"\"\n",
    "        self.document_topic_prob = np.random.random_sample((self.number_of_documents, number_of_topics))\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob) # P(z | d)\n",
    "\n",
    "        self.topic_word_prob = np.random.random_sample((number_of_topics, len(self.vocabulary)))\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob) # P(w | z)\n",
    "\n",
    "    def initialize_uniformly(self, number_of_topics):\n",
    "        \"\"\"\n",
    "        Initializes the matrices: self.document_topic_prob and self.topic_word_prob with a uniform\n",
    "        probability distribution. This is used for testing purposes.\n",
    "\n",
    "        DO NOT CHANGE THIS FUNCTION\n",
    "        \"\"\"\n",
    "        self.document_topic_prob = np.ones((self.number_of_documents, number_of_topics))\n",
    "        self.document_topic_prob = normalize(self.document_topic_prob) # P(z | d)\n",
    "\n",
    "        self.topic_word_prob = np.ones((number_of_topics, len(self.vocabulary)))\n",
    "        self.topic_word_prob = normalize(self.topic_word_prob) # P(w | z)\n",
    "\n",
    "    def initialize(self, number_of_topics, random=False):\n",
    "        \"\"\" Call the functions to initialize the matrices document_topic_prob and topic_word_prob\n",
    "        \"\"\"\n",
    "        print(\"Initializing...\")\n",
    "\n",
    "        if random:\n",
    "            self.initialize_randomly(number_of_topics)\n",
    "        else:\n",
    "            self.initialize_uniformly(number_of_topics)\n",
    "\n",
    "    def expectation_step(self):\n",
    "        \"\"\" The E-step updates P(z | w, d)\n",
    "        \"\"\"\n",
    "        print(\"E step:\")\n",
    "\n",
    "        for i in range(self.topic_prob.shape[1]):\n",
    "            self.topic_prob[:, i, :] =  np.outer(self.document_topic_prob[:, i], self.topic_word_prob[i, :])\n",
    "            self.topic_prob[:, i, :] /= np.matmul(self.document_topic_prob, self.topic_word_prob)\n",
    "\n",
    "    def maximization_step(self, number_of_topics):\n",
    "        \"\"\" The M-step updates P(w | z) and P(z | d)\n",
    "        \"\"\"\n",
    "        print(\"M step:\")\n",
    "\n",
    "        # update P(w | z)\n",
    "        for i in range(number_of_topics):\n",
    "            self.topic_word_prob[i, :] = np.diag(np.matmul(np.transpose(self.term_doc_matrix), self.topic_prob[:, i, :]))\n",
    "            self.topic_word_prob[i, :] /= np.sum(self.topic_word_prob[i, :])\n",
    "\n",
    "        # update P(z | d)\n",
    "        for i in range(number_of_topics):\n",
    "            self.document_topic_prob[:, i] = np.diag(np.matmul(self.term_doc_matrix, np.transpose(self.topic_prob[:, i, :])))\n",
    "        self.document_topic_prob /= np.sum(self.document_topic_prob, axis = 1)[:, None]\n",
    "\n",
    "    def calculate_likelihood(self, number_of_topics):\n",
    "        \"\"\" Calculate the current log-likelihood of the model using\n",
    "        the model's updated probability matrices\n",
    "\n",
    "        Append the calculated log-likelihood to self.likelihoods\n",
    "        \"\"\"\n",
    "        self.likelihoods.append(np.sum(np.multiply(self.term_doc_matrix, np.log(np.matmul(self.document_topic_prob, self.topic_word_prob)))))\n",
    "        return self.likelihoods[-1]\n",
    "\n",
    "    def plsa(self, number_of_topics, max_iter, epsilon):\n",
    "\n",
    "        \"\"\"\n",
    "        Model topics.\n",
    "        \"\"\"\n",
    "        print (\"EM iteration begins...\")\n",
    "\n",
    "        # build term-doc matrix\n",
    "        self.build_term_doc_matrix()\n",
    "\n",
    "        # Create the counter arrays.\n",
    "\n",
    "        # P(z | d, w)\n",
    "        self.topic_prob = np.zeros([self.number_of_documents, number_of_topics, self.vocabulary_size], dtype=np.float)\n",
    "\n",
    "        # P(z | d) P(w | z)\n",
    "        self.initialize(number_of_topics, random=True)\n",
    "\n",
    "        # Run the EM algorithm\n",
    "        current_likelihood = 0.0\n",
    "\n",
    "        for iteration in range(max_iter):\n",
    "            print(\"Iteration #\" + str(iteration + 1) + \"...\")\n",
    "            self.expectation_step()\n",
    "            self.maximization_step(number_of_topics)\n",
    "            previous_likelihood = current_likelihood\n",
    "            current_likelihood = self.calculate_likelihood(number_of_topics)\n",
    "            if np.abs(current_likelihood - previous_likelihood) < epsilon:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(\"./data/nyt_corpus/data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start 5:41 PM, end 6:08 PM\n",
    "# start 6:10 PM, end 6:37 PM\n",
    "# start 8:38 PM, end 8:48 PM\n",
    "corpus.build_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.build_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM iteration begins...\n",
      "Initializing...\n",
      "Iteration #1...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #2...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #3...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #4...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #5...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #6...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #7...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #8...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #9...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #10...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #11...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #12...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #13...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #14...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #15...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #16...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #17...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #18...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #19...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #20...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #21...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #22...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #23...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #24...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #25...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #26...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #27...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #28...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #29...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #30...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #31...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #32...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #33...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #34...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #35...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #36...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #37...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #38...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #39...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #40...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #41...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #42...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #43...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #44...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #45...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #46...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #47...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #48...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #49...\n",
      "E step:\n",
      "M step:\n",
      "Iteration #50...\n",
      "E step:\n",
      "M step:\n"
     ]
    }
   ],
   "source": [
    "# start 7:14 PM, end 8:08 PM\n",
    "# start 8:51 PM, end 9:05 PM\n",
    "number_of_topics = 2\n",
    "max_iterations = 50\n",
    "epsilon = 0.001\n",
    "corpus.plsa(number_of_topics, max_iterations, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.Series(corpus.doc_timestamps).astype(\"datetime64\")[pd.Series(corpus.doc_timestamps).astype(\"datetime64\") == AAPL[\"Date\"].astype(\"datetime64\")[50]].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.append(corpus.document_topic_prob[d].sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.stack(df, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.append(df,corpus.document_topic_prob[d].sum(axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2000-09-13 00:00:00')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AAPL[\"Date\"].astype(\"datetime64\")[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AAPL[\"Date\"].astype(\"datetime64\")[AAPL[\"Date\"].astype(\"datetime64\") == pd.Series(corpus.doc_timestamps).astype(\"datetime64\")[0]].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_level_causality(document_topic_prob, doc_timestamps, time_series, ts_timestamps, tn):\n",
    "    time_series_sub = []\n",
    "    ts = []\n",
    "    for k, t in enumerate(ts_timestamps):\n",
    "        doc_i = doc_timestamps[doc_timestamps == t].index\n",
    "        if doc_i.any():\n",
    "            time_series_sub.append(time_series.iloc[k])\n",
    "            ts.append(document_topic_prob[doc_i].sum(axis = 0))\n",
    "    ts = np.stack(ts, axis = 1)\n",
    "    for topic in range(ts.shape[0]):\n",
    "        gc_res = grangercausalitytests(close, 5)\n",
    "        p_vals = []\n",
    "        for i in range(1, len(gc_res) + 1):\n",
    "            p_vals.append(gc_res[i][0]['params_ftest'][1])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meet',\n",
       " 'shetland',\n",
       " 'perennially',\n",
       " 'missionary',\n",
       " 'valedictory',\n",
       " 'concerto',\n",
       " 'enumerate',\n",
       " 'mandate',\n",
       " 'barnstorm',\n",
       " 'delgado',\n",
       " 'posterity',\n",
       " 'peanut',\n",
       " 'elaboration',\n",
       " 'lamentable',\n",
       " 'ocher',\n",
       " 'questionable',\n",
       " 'pretext',\n",
       " 'bennett',\n",
       " 'calista',\n",
       " 'patella',\n",
       " 'result',\n",
       " 'arlington',\n",
       " 'crawford',\n",
       " 'elicit',\n",
       " 'extraordinary',\n",
       " 'overblown',\n",
       " 'carbine',\n",
       " 'enunciate',\n",
       " 'wilma',\n",
       " 'beam',\n",
       " 'snigger',\n",
       " 'cowardly',\n",
       " 'mindy',\n",
       " 'recession',\n",
       " 'centre',\n",
       " 'stuyvesant',\n",
       " 'serrano',\n",
       " 'chapin',\n",
       " 'customize',\n",
       " 'decisively',\n",
       " 'outlast',\n",
       " 'tenderloin',\n",
       " 'analogous',\n",
       " 'bourbon',\n",
       " 'bogged',\n",
       " 'engender',\n",
       " 'meal',\n",
       " 'supagroup',\n",
       " 'unremitting',\n",
       " 'omit',\n",
       " 'trombonist',\n",
       " 'snakeskin',\n",
       " 'helm',\n",
       " 'gallon',\n",
       " 'desouza',\n",
       " 'distinction',\n",
       " 'coney',\n",
       " 'lockheed',\n",
       " 'ibrahim',\n",
       " 'weariness',\n",
       " 'wont',\n",
       " 'velazquez',\n",
       " 'zedillo',\n",
       " 'biogen',\n",
       " 'vengeance',\n",
       " 'taint',\n",
       " 'fringe',\n",
       " 'heap',\n",
       " 'cryptic',\n",
       " 'crewman',\n",
       " 'E twenty four',\n",
       " 'exceptionally',\n",
       " 'yogi',\n",
       " 'auchincloss',\n",
       " 'sideline',\n",
       " 'nineteen sixteen',\n",
       " 'clergyman',\n",
       " 'schakowsky',\n",
       " 'unfair',\n",
       " 'handy',\n",
       " 'mail',\n",
       " 'porch',\n",
       " 'persuasion',\n",
       " 'cab',\n",
       " 'more',\n",
       " 'chore',\n",
       " 'zanzibar',\n",
       " 'clearly',\n",
       " 'intellectually',\n",
       " 'method',\n",
       " 'superstructure',\n",
       " 'degrade',\n",
       " 'artifact',\n",
       " 'enquirer',\n",
       " 'webvan',\n",
       " 'downplay',\n",
       " 'penn',\n",
       " 'cluster',\n",
       " 'weaponry',\n",
       " 'feverishly',\n",
       " 'steeper',\n",
       " 'string',\n",
       " 'bengal',\n",
       " 'anomalous',\n",
       " 'redeployment',\n",
       " 'davy',\n",
       " 'mom',\n",
       " 'catch',\n",
       " 'spit',\n",
       " 'chirp',\n",
       " 'generalization',\n",
       " 'petulance',\n",
       " 'unresponsive',\n",
       " 'fatter',\n",
       " 'homogeneity',\n",
       " 'chord',\n",
       " 'bridgeport',\n",
       " 'orientation',\n",
       " 'narcissism',\n",
       " 'gospel',\n",
       " 'electric',\n",
       " 'hoffa',\n",
       " 'restitution',\n",
       " 'observatory',\n",
       " 'imply',\n",
       " 'juggernaut',\n",
       " 'safety',\n",
       " 'unenthusiastic',\n",
       " 'torah',\n",
       " 'searcher',\n",
       " 'schwartz',\n",
       " 'scorecard',\n",
       " 'captivate',\n",
       " 'group',\n",
       " 'shalt',\n",
       " 'rand',\n",
       " 'fisherman',\n",
       " 'transplant',\n",
       " 'nibble',\n",
       " 'stringent',\n",
       " 'humdrum',\n",
       " 'ancestral',\n",
       " 'outdress',\n",
       " 'clergy',\n",
       " 'determinedly',\n",
       " 'heir',\n",
       " 'estrogen',\n",
       " 'hasten',\n",
       " 'apart',\n",
       " 'chen',\n",
       " 'safeguard',\n",
       " 'marvelous',\n",
       " 'stridency',\n",
       " 'feasibility',\n",
       " 'enthral',\n",
       " 'scrimmage',\n",
       " 'igloo',\n",
       " 'chancellor',\n",
       " 'twinkle',\n",
       " 'cargo',\n",
       " 'parader',\n",
       " 'bracken',\n",
       " 'nonissue',\n",
       " 'gazelle',\n",
       " 'hyperbolic',\n",
       " 'bosnian',\n",
       " 'oral',\n",
       " 'corrupt',\n",
       " 'ishtar',\n",
       " 'day',\n",
       " 'shamalekh',\n",
       " 'shiite',\n",
       " 'defiantly',\n",
       " 'six hundred',\n",
       " 'postmark',\n",
       " 'biographical',\n",
       " 'wastefully',\n",
       " 'furious',\n",
       " 'four hundred and eighty',\n",
       " 'easiest',\n",
       " 'eclectic',\n",
       " 'nudge',\n",
       " 'disabuse',\n",
       " 'anticipatory',\n",
       " 'glacier',\n",
       " 'attitude',\n",
       " 'qualification',\n",
       " 'graze',\n",
       " 'appall',\n",
       " 'clintonites',\n",
       " 'disarmament',\n",
       " 'abortive',\n",
       " 'scatological',\n",
       " 'slouch',\n",
       " 'rack',\n",
       " 'elisabeth',\n",
       " 'princess',\n",
       " 'metamorphose',\n",
       " 'waldorf',\n",
       " 'slayer',\n",
       " 'urgently',\n",
       " 'zone',\n",
       " 'prod',\n",
       " 'remediable',\n",
       " 'cooper',\n",
       " 'noah',\n",
       " 'polly',\n",
       " 'F six',\n",
       " 'albuquerque',\n",
       " 'doro',\n",
       " 'arouse',\n",
       " 'radiology',\n",
       " 'carlin',\n",
       " 'gram',\n",
       " 'moralistic',\n",
       " 'film',\n",
       " 'meritless',\n",
       " 'toddler',\n",
       " 'masquerade',\n",
       " 'sheriff',\n",
       " 'cough',\n",
       " 'road',\n",
       " 'aftereffect',\n",
       " 'oak',\n",
       " 'prescription',\n",
       " 'bird',\n",
       " 'rob',\n",
       " 'simchat',\n",
       " 'designer',\n",
       " 'underneath',\n",
       " 'riposte',\n",
       " 'rucker',\n",
       " 'patty',\n",
       " 'cito',\n",
       " 'slang',\n",
       " 'concord',\n",
       " 'keeper',\n",
       " 'outgunned',\n",
       " 'offstage',\n",
       " 'popcorn',\n",
       " 'aggrieve',\n",
       " 'differently',\n",
       " 'bride',\n",
       " 'whinge',\n",
       " 'anna',\n",
       " 'shamelessly',\n",
       " 'feel',\n",
       " 'bleecker',\n",
       " 'multisyllable',\n",
       " 'nightly',\n",
       " 'nimble',\n",
       " 'incapable',\n",
       " 'upraise',\n",
       " 'impossible',\n",
       " 'quash',\n",
       " 'joyful',\n",
       " 'greatly',\n",
       " 'horribly',\n",
       " 'treacherous',\n",
       " 'preamble',\n",
       " 'police',\n",
       " 'middle',\n",
       " 'monkey',\n",
       " 'spray',\n",
       " 'spiritual',\n",
       " 'perfect',\n",
       " 'sauce',\n",
       " 'slot',\n",
       " 'baronial',\n",
       " 'noir',\n",
       " 'exploratory',\n",
       " 'wiezien',\n",
       " 'hepatitis',\n",
       " 'artificially',\n",
       " 'revert',\n",
       " 'archie',\n",
       " 'pontificate',\n",
       " 'counterattack',\n",
       " 'sophisticate',\n",
       " 'fossella',\n",
       " 'staffer',\n",
       " 'abbreviation',\n",
       " 'chutzpah',\n",
       " 'acknowledgment',\n",
       " 'cps',\n",
       " 'enunciator',\n",
       " 'clintonesque',\n",
       " 'crumple',\n",
       " 'larva',\n",
       " 'cadre',\n",
       " 'fremont',\n",
       " 'rotter',\n",
       " 'action',\n",
       " 'ritualistic',\n",
       " 'misidentifed',\n",
       " 'polio',\n",
       " 'ayatollah',\n",
       " 'offshoot',\n",
       " 'skyline',\n",
       " 'factionalism',\n",
       " 'sal',\n",
       " 'tire',\n",
       " 'overshadow',\n",
       " 'kwajalein',\n",
       " 'artillery',\n",
       " 'party',\n",
       " 'forecaster',\n",
       " 'strain',\n",
       " 'contradictory',\n",
       " 'ration',\n",
       " 'flotsam',\n",
       " 'envoy',\n",
       " 'lack',\n",
       " 'armitage',\n",
       " 'stoke',\n",
       " 'solitary',\n",
       " 'freshness',\n",
       " 'todd',\n",
       " 'mitchell',\n",
       " 'token',\n",
       " 'drumbeat',\n",
       " 'service',\n",
       " 'untrue',\n",
       " 'mayer',\n",
       " 'television',\n",
       " 'floridian',\n",
       " 'vladimir',\n",
       " 'solar',\n",
       " 'quip',\n",
       " 'cruise',\n",
       " 'portrayal',\n",
       " 'beleaguer',\n",
       " 'guzzler',\n",
       " 'flash',\n",
       " 'delicious',\n",
       " 'twenty twenty four',\n",
       " 'igor',\n",
       " 'bake',\n",
       " 'insistent',\n",
       " 'implosion',\n",
       " 'two hundred and nine',\n",
       " 'earth',\n",
       " 'heroic',\n",
       " 'amiability',\n",
       " 'veracity',\n",
       " 'intervene',\n",
       " 'unbounded',\n",
       " 'databank',\n",
       " 'reflux',\n",
       " 'jeopardy',\n",
       " 'vie',\n",
       " 'joy',\n",
       " 'ing',\n",
       " 'trade',\n",
       " 'mushroom',\n",
       " 'barney',\n",
       " 'magnet',\n",
       " 'generalize',\n",
       " 'inherit',\n",
       " 'nothing',\n",
       " 'truism',\n",
       " 'briefly',\n",
       " 'blurb',\n",
       " 'ruben',\n",
       " 'moline',\n",
       " 'bryn',\n",
       " 'rout',\n",
       " 'grandparent',\n",
       " 'three hundred and fifty',\n",
       " 'insignificant',\n",
       " 'wring',\n",
       " 'sixty',\n",
       " 'stigmatize',\n",
       " 'locality',\n",
       " 'chandelle',\n",
       " 'cascade',\n",
       " 'enclave',\n",
       " 'dispatch',\n",
       " 'deeper',\n",
       " 'uncensored',\n",
       " 'manufacture',\n",
       " 'a ten',\n",
       " 'big',\n",
       " 'unfairness',\n",
       " 'rung',\n",
       " 'nephew',\n",
       " 'liar',\n",
       " 'ironclad',\n",
       " 'deviate',\n",
       " 'reauthorization',\n",
       " 'snicker',\n",
       " 'incentive',\n",
       " 'watchdog',\n",
       " 'cucumber',\n",
       " 'throat',\n",
       " 'greatness',\n",
       " 'yellow',\n",
       " 'solitaire',\n",
       " 'obliquely',\n",
       " 'devise',\n",
       " 'jurisprudence',\n",
       " 'deject',\n",
       " 'league',\n",
       " 'trainmaking',\n",
       " 'prospectively',\n",
       " 'involvement',\n",
       " 'vermont',\n",
       " 'delegation',\n",
       " 'trek',\n",
       " 'eschew',\n",
       " 'molehill',\n",
       " 'productivity',\n",
       " 'chevron',\n",
       " 'estimation',\n",
       " 'antipoverty',\n",
       " 'minor',\n",
       " 'frannie',\n",
       " 'backbreaking',\n",
       " 'redivision',\n",
       " 'fierce',\n",
       " 'put',\n",
       " 'pleasantry',\n",
       " 'harbor',\n",
       " 'huntington',\n",
       " 'gentlemanliness',\n",
       " 'seethe',\n",
       " 'individuality',\n",
       " 'teenage',\n",
       " 'federalist',\n",
       " 'abandonment',\n",
       " 'event',\n",
       " 'classroom',\n",
       " 'criterion',\n",
       " 'romulus',\n",
       " 'dissolution',\n",
       " 'two hundred and forty two',\n",
       " 'grave',\n",
       " 'aerobic',\n",
       " 'velella',\n",
       " 'exception',\n",
       " 'easygoing',\n",
       " 'outer',\n",
       " 'aspersion',\n",
       " 'sotheby',\n",
       " 'suzanne',\n",
       " 'sponsor',\n",
       " 'unfavorably',\n",
       " 'rouse',\n",
       " 'tossup',\n",
       " 'nontraditional',\n",
       " 'tauziat',\n",
       " 'baxter',\n",
       " 'small',\n",
       " 'reserve',\n",
       " 'empty',\n",
       " 'semiconductor',\n",
       " 'kroger',\n",
       " 'stop',\n",
       " 'doll',\n",
       " 'latecomer',\n",
       " 'salvadore',\n",
       " 'clay',\n",
       " 'lyndon',\n",
       " 'honorability',\n",
       " 'feather',\n",
       " 'pavel',\n",
       " 'wither',\n",
       " 'neutral',\n",
       " 'tent',\n",
       " 'affectation',\n",
       " 'progress',\n",
       " 'remarkable',\n",
       " 'free',\n",
       " 'prudence',\n",
       " 'selma',\n",
       " 'rehabilitation',\n",
       " 'hike',\n",
       " 'soho',\n",
       " 'endure',\n",
       " 'buckskin',\n",
       " 'surrender',\n",
       " 'rebound',\n",
       " 'gayle',\n",
       " 'website',\n",
       " 'defend',\n",
       " 'unresolved',\n",
       " 'retrieve',\n",
       " 'belligerently',\n",
       " 'evident',\n",
       " 'instinct',\n",
       " 'medal',\n",
       " 'penetrate',\n",
       " 'swedish',\n",
       " 'abbreviate',\n",
       " 'thematically',\n",
       " 'jude',\n",
       " 'F five',\n",
       " 'rebuke',\n",
       " 'due',\n",
       " 'personalize',\n",
       " 'mattie',\n",
       " 'predawn',\n",
       " 'merely',\n",
       " 'legendary',\n",
       " 'example',\n",
       " 'suffix',\n",
       " 'flummox',\n",
       " 'anxiously',\n",
       " 'spotty',\n",
       " 'slow',\n",
       " 'secrecy',\n",
       " 'diameter',\n",
       " 'shelburne',\n",
       " 'generic',\n",
       " 'O Z',\n",
       " 'inspirational',\n",
       " 'belt',\n",
       " 'cotton',\n",
       " 'deathbed',\n",
       " 'blackout',\n",
       " 'magician',\n",
       " 'jersey',\n",
       " 'nygren',\n",
       " 'retire',\n",
       " 'computerize',\n",
       " 'disingenuous',\n",
       " 'flexible',\n",
       " 'rain',\n",
       " 'hsi',\n",
       " 'nearby',\n",
       " 'avascular',\n",
       " 'transform',\n",
       " 'supersonic',\n",
       " 'ha',\n",
       " 'lopez',\n",
       " 'painless',\n",
       " 'ferraris',\n",
       " 'player',\n",
       " 'kitchen',\n",
       " 'wool',\n",
       " 'turmoil',\n",
       " 'montreal',\n",
       " 'cruikshank',\n",
       " 'elect',\n",
       " 'placid',\n",
       " 'mcclellan',\n",
       " 'pistol',\n",
       " 'zeal',\n",
       " 'way',\n",
       " 'carroll',\n",
       " 'sir',\n",
       " 'corsage',\n",
       " 'desk',\n",
       " 'motionless',\n",
       " 'bergonzi',\n",
       " 'moor',\n",
       " 'rubicam',\n",
       " 'mcclusky',\n",
       " 'be',\n",
       " 'one hundred and sixty two',\n",
       " 'excise',\n",
       " 'implore',\n",
       " 'huh',\n",
       " 'cub',\n",
       " 'falco',\n",
       " 'webcam',\n",
       " 'hand',\n",
       " 'nonliving',\n",
       " 'ivanovich',\n",
       " 'sinatra',\n",
       " 'sin',\n",
       " 'nonchalant',\n",
       " 'western',\n",
       " 'ambersons',\n",
       " 'evidence',\n",
       " 'alamo',\n",
       " 'defector',\n",
       " 'modernize',\n",
       " 'angola',\n",
       " 'brunt',\n",
       " 'tabloid',\n",
       " 'threat',\n",
       " 'decision',\n",
       " 'poker',\n",
       " 'humiliate',\n",
       " 'flirt',\n",
       " 'flood',\n",
       " 'authoritative',\n",
       " 'highly',\n",
       " 'stability',\n",
       " 'roommate',\n",
       " 'gladly',\n",
       " 'saban',\n",
       " 'unpredictably',\n",
       " 'embezzlement',\n",
       " 'alma',\n",
       " 'transient',\n",
       " 'insular',\n",
       " 'neurological',\n",
       " 'inevitability',\n",
       " 'deterioration',\n",
       " 'onion',\n",
       " 'clockwork',\n",
       " 'prowl',\n",
       " 'socorro',\n",
       " 'barbershop',\n",
       " 'hongbao',\n",
       " 'encourage',\n",
       " 'lifelong',\n",
       " 'tracy',\n",
       " 'stink',\n",
       " 'parry',\n",
       " 'remark',\n",
       " 'consist',\n",
       " 'contraception',\n",
       " 'lapel',\n",
       " 'krakauer',\n",
       " 'reduction',\n",
       " 'messy',\n",
       " 'quantity',\n",
       " 'plank',\n",
       " 'rectory',\n",
       " 'flourish',\n",
       " 'festivity',\n",
       " 'fry',\n",
       " 'except',\n",
       " 'fluff',\n",
       " 'miss',\n",
       " 'fanatical',\n",
       " 'neurosurgery',\n",
       " 'tile',\n",
       " 'staff',\n",
       " 'canard',\n",
       " 'dip',\n",
       " 'coordinator',\n",
       " 'hospitalization',\n",
       " 'purely',\n",
       " 'pedantic',\n",
       " 'clock',\n",
       " 'epitomize',\n",
       " 'five hundred and fifteen',\n",
       " 'infinitely',\n",
       " 'backfire',\n",
       " 'open',\n",
       " 'hook',\n",
       " 'jay',\n",
       " 'syracuse',\n",
       " 'spearhead',\n",
       " 'undaunted',\n",
       " 'endorsement',\n",
       " 'though',\n",
       " 'debut',\n",
       " 'barb',\n",
       " 'happiness',\n",
       " 'cathy',\n",
       " 'convene',\n",
       " 'afterwards',\n",
       " 'clearer',\n",
       " 'mendicant',\n",
       " 'instant',\n",
       " 'goofy',\n",
       " 'overlook',\n",
       " 'dame',\n",
       " 'desecration',\n",
       " 'nonvoter',\n",
       " 'outweigh',\n",
       " 'gorton',\n",
       " 'practical',\n",
       " 'item',\n",
       " 'guttural',\n",
       " 'post',\n",
       " 'education',\n",
       " 'easier',\n",
       " 'caller',\n",
       " 'hamster',\n",
       " 'adjournment',\n",
       " 'backside',\n",
       " 'redder',\n",
       " 'incorrectly',\n",
       " 'cripple',\n",
       " 'audition',\n",
       " 'seven',\n",
       " 'jiggy',\n",
       " 'guru',\n",
       " 'limp',\n",
       " 'loathe',\n",
       " 'unsustainable',\n",
       " 'wheelchair',\n",
       " 'dependency',\n",
       " 'vestige',\n",
       " 'giddy',\n",
       " 'how',\n",
       " 'connally',\n",
       " 'westlanders',\n",
       " 'package',\n",
       " 'boeing',\n",
       " 'cambridge',\n",
       " 'jenny',\n",
       " 'twenty fifteen',\n",
       " 'bennet',\n",
       " 'tale',\n",
       " 'one hundred and sixty',\n",
       " 'steadiness',\n",
       " 'darryl',\n",
       " 'foe',\n",
       " 'pedestrian',\n",
       " 'browne',\n",
       " 'unrepentant',\n",
       " 'journey',\n",
       " 'scrupulously',\n",
       " 'optimistic',\n",
       " 'dissection',\n",
       " 'wetstone',\n",
       " 'tech',\n",
       " 'stabilize',\n",
       " 'unprotected',\n",
       " 'lowell',\n",
       " 'understate',\n",
       " 'viciousness',\n",
       " 'phantom',\n",
       " 'palace',\n",
       " 'macroeconomic',\n",
       " 'welch',\n",
       " 'risky',\n",
       " 'underwood',\n",
       " 'nary',\n",
       " 'a one',\n",
       " 'restrain',\n",
       " 'legend',\n",
       " 'arabia',\n",
       " 'solve',\n",
       " 'destabilize',\n",
       " 'verbally',\n",
       " 'incorporate',\n",
       " 'allure',\n",
       " 'marinate',\n",
       " 'eighteenth',\n",
       " 'accouterment',\n",
       " 'appearance',\n",
       " 'fernando',\n",
       " 'cushwa',\n",
       " 'stabilizer',\n",
       " 'linger',\n",
       " 'mediation',\n",
       " 'lexicographic',\n",
       " 'electoral',\n",
       " 'statehood',\n",
       " 'devoutly',\n",
       " 'lorraine',\n",
       " 'bibi',\n",
       " 'wordy',\n",
       " 'griffin',\n",
       " 'archer',\n",
       " 'pennant',\n",
       " 'mccullagh',\n",
       " 'supporter',\n",
       " 'tame',\n",
       " 'arrival',\n",
       " 'seven hundred and fifty',\n",
       " 'trooper',\n",
       " 'bright',\n",
       " 'fabiani',\n",
       " 'nowhere',\n",
       " 'declension',\n",
       " 'recreate',\n",
       " 'around',\n",
       " 'spotlight',\n",
       " 'coastline',\n",
       " 'journalist',\n",
       " 'syllable',\n",
       " 'steam',\n",
       " 'shovel',\n",
       " 'sleepy',\n",
       " 'thurgood',\n",
       " 'digestible',\n",
       " 'rotenberg',\n",
       " 'definition',\n",
       " 'twilight',\n",
       " 'sweetly',\n",
       " 'inevitably',\n",
       " 'californian',\n",
       " 'colleague',\n",
       " 'sixteen',\n",
       " 'rictus',\n",
       " 'mystery',\n",
       " 'physiologist',\n",
       " 'internment',\n",
       " 'pabulum',\n",
       " 'fictitious',\n",
       " 'saturation',\n",
       " 'mispronounce',\n",
       " 'thunderer',\n",
       " 'puritan',\n",
       " 'rebuff',\n",
       " 'irrepressible',\n",
       " 'force',\n",
       " 'piney',\n",
       " 'veto',\n",
       " 'pew',\n",
       " 'adversary',\n",
       " 'dastardly',\n",
       " 'secretive',\n",
       " 'voucher',\n",
       " 'tun',\n",
       " 'blithely',\n",
       " 'noose',\n",
       " 'receptive',\n",
       " 'officeholder',\n",
       " 'kansa',\n",
       " 'pack',\n",
       " 'anxiety',\n",
       " 'industrial',\n",
       " 'besides',\n",
       " 'disquiet',\n",
       " 'large',\n",
       " 'assent',\n",
       " 'championship',\n",
       " 'dilemma',\n",
       " 'dennis',\n",
       " 'forthright',\n",
       " 'eyewitness',\n",
       " 'seventy three',\n",
       " 'tube',\n",
       " 'arraign',\n",
       " 'almost',\n",
       " 'holy',\n",
       " 'four hundred and four',\n",
       " 'bossman',\n",
       " 'baptistery',\n",
       " 'spinner',\n",
       " 'rupture',\n",
       " 'litigation',\n",
       " 'tad',\n",
       " 'arrangement',\n",
       " 'zorrillas',\n",
       " 'relic',\n",
       " 'reply',\n",
       " 'hygiene',\n",
       " 'rightfully',\n",
       " 'E nine',\n",
       " 'kathy',\n",
       " 'broker',\n",
       " 'brood',\n",
       " 'elderly',\n",
       " 'juicy',\n",
       " 'extensively',\n",
       " 'george',\n",
       " 'rostenkowski',\n",
       " 'disaffect',\n",
       " 'rapist',\n",
       " 'chabot',\n",
       " 'bean',\n",
       " 'pitfall',\n",
       " 'abet',\n",
       " 'inexperience',\n",
       " 'counterintuitive',\n",
       " 'embody',\n",
       " 'skinhead',\n",
       " 'cornell',\n",
       " 'unabridged',\n",
       " 'utter',\n",
       " 'waspish',\n",
       " 'irreducible',\n",
       " 'hope',\n",
       " 'gibber',\n",
       " 'punt',\n",
       " 'te',\n",
       " 'medford',\n",
       " 'sidestep',\n",
       " 'recast',\n",
       " 'timer',\n",
       " 'room',\n",
       " 'precision',\n",
       " 'dog',\n",
       " 'acquit',\n",
       " 'crocketed',\n",
       " 'plymouth',\n",
       " 'care',\n",
       " 'font',\n",
       " 'tricky',\n",
       " 'denial',\n",
       " 'early',\n",
       " 'eulogy',\n",
       " 'midget',\n",
       " 'shaft',\n",
       " 'guacamole',\n",
       " 'secular',\n",
       " 'smile',\n",
       " 'disintegrate',\n",
       " 'ronald',\n",
       " 'platonic',\n",
       " 'gall',\n",
       " 'firehouse',\n",
       " 'unabashed',\n",
       " 'three hundred and thirty two',\n",
       " 'sturgeon',\n",
       " 'pastel',\n",
       " 'strangeness',\n",
       " 'bone',\n",
       " 'inconceivable',\n",
       " 'deluge',\n",
       " 'hollingsworth',\n",
       " 'textured',\n",
       " 'mainstream',\n",
       " 'c twenty three',\n",
       " 'ide',\n",
       " 'famed',\n",
       " 'denizen',\n",
       " 'versus',\n",
       " 'abidjan',\n",
       " 'cavilleri',\n",
       " 'profiteer',\n",
       " 'fulfil',\n",
       " 'shelby',\n",
       " 'subsequently',\n",
       " 'sodomy',\n",
       " 'keynote',\n",
       " 'compute',\n",
       " 'regroup',\n",
       " 'gridlock',\n",
       " 'matrix',\n",
       " 'ordinariness',\n",
       " 'skit',\n",
       " 'spousal',\n",
       " 'clemente',\n",
       " 'jaffna',\n",
       " 'june',\n",
       " 'omar',\n",
       " 'update',\n",
       " 'B four',\n",
       " 'grandchild',\n",
       " 'bankroll',\n",
       " 'member',\n",
       " 'sketch',\n",
       " 'wilderness',\n",
       " 'surgery',\n",
       " 'ahrens',\n",
       " 'separatism',\n",
       " 'bendheim',\n",
       " 'caliber',\n",
       " 'receiver',\n",
       " 'F three',\n",
       " 'unconscionable',\n",
       " 'disdain',\n",
       " 'easeful',\n",
       " 'quadrant',\n",
       " 'darlene',\n",
       " 'advancement',\n",
       " 'hendrick',\n",
       " 'ricki',\n",
       " 'cooler',\n",
       " 'tremble',\n",
       " 'dee',\n",
       " 'jauntily',\n",
       " 'twenty seven',\n",
       " 'godfrey',\n",
       " 'vampire',\n",
       " 'microprocessing',\n",
       " 'steely',\n",
       " 'unicorn',\n",
       " 'wander',\n",
       " 'cabaret',\n",
       " 'nine hundred and eleven',\n",
       " 'two hundred and thirty',\n",
       " 'wrist',\n",
       " 'restive',\n",
       " 'jumble',\n",
       " 'meaningfully',\n",
       " 'unvocalized',\n",
       " 'devil',\n",
       " 'saxophonist',\n",
       " 'holder',\n",
       " 'omega',\n",
       " 'megahit',\n",
       " 'accelerate',\n",
       " 'ashcroft',\n",
       " 'incorrigible',\n",
       " 'jahnke',\n",
       " 'outlying',\n",
       " 'administration',\n",
       " 'stammer',\n",
       " 'iraqi',\n",
       " 'aside',\n",
       " 'achievable',\n",
       " 'penchant',\n",
       " 'caesar',\n",
       " 'prelude',\n",
       " 'dean',\n",
       " 'coop',\n",
       " 'westchester',\n",
       " 'auschwitz',\n",
       " 'smuggle',\n",
       " 'sandra',\n",
       " 'scuttle',\n",
       " 'gauche',\n",
       " 'joanna',\n",
       " 'santa',\n",
       " 'gridlocked',\n",
       " 'forrister',\n",
       " 'strongest',\n",
       " ...]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(corpus.document_topic_prob, axis=1) # P(z | d) [num documents, num topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.topic_word_prob # P(w | z)  [num topics, num words in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9.72766790e-01, 1.00000000e+00, 7.71679371e-01, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [2.72332101e-02, 7.51023205e-53, 2.28320629e-01, ...,\n",
       "         3.56863371e-47, 2.70435982e-29, 6.96381040e-32]],\n",
       "\n",
       "       [[9.68082447e-01, 1.00000000e+00, 7.41594437e-01, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [3.19175526e-02, 8.84464672e-53, 2.58405563e-01, ...,\n",
       "         4.20270695e-47, 3.18486927e-29, 8.20113711e-32]],\n",
       "\n",
       "       [[9.19375996e-01, 1.00000000e+00, 5.18993308e-01, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [8.06240038e-02, 2.35252613e-52, 4.81006692e-01, ...,\n",
       "         1.11784882e-46, 8.47121248e-29, 2.18136348e-31]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[4.05182041e-01, 1.00000000e+00, 6.05509797e-02, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [5.94817959e-01, 3.93819429e-51, 9.39449020e-01, ...,\n",
       "         1.87131007e-45, 1.41810457e-27, 3.65166325e-30]],\n",
       "\n",
       "       [[2.33270459e-01, 1.00000000e+00, 2.79816833e-02, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [7.66729541e-01, 8.81750485e-51, 9.72018317e-01, ...,\n",
       "         4.18980995e-45, 3.17509575e-27, 8.17597000e-30]],\n",
       "\n",
       "       [[4.30658760e-01, 1.00000000e+00, 6.67914959e-02, ...,\n",
       "         1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        [5.69341240e-01, 3.54652163e-51, 9.33208504e-01, ...,\n",
       "         1.68519914e-45, 1.27706715e-27, 3.28848749e-30]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.topic_prob # P(z | d, w) [num documents, num topics, num words in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
